{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX_xl9Jj_Ov6",
        "outputId": "d7e6e510-6597-40a6-be1e-c4c09b5b0cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scikit-learn tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_dataset(page_dataset, for_inference):\n",
        "    labeled_text_dataset = []\n",
        "    for page in page_dataset:\n",
        "        page_words = page[\"representativeData\"][\"page_data_words\"]\n",
        "\n",
        "        geo_dictionary = {}\n",
        "        if not for_inference:\n",
        "            page_answers = page.get(\"answers\")\n",
        "            for page_answer in page_answers[0][\"answer\"]:\n",
        "                geo_label = page_answer[\"id\"]\n",
        "                for geo_part in page_answer[\"data\"]:\n",
        "                    for index in range(geo_part[\"start\"], geo_part[\"end\"]):\n",
        "                        geo_dictionary[index] = geo_label\n",
        "\n",
        "        labeled_text = []\n",
        "        for word_index, word in enumerate(page_words):\n",
        "            word_label = \"0\" if for_inference else geo_dictionary.get(word_index, \"O\")\n",
        "            labeled_text.append((word, word_label))\n",
        "\n",
        "        if not for_inference:\n",
        "            labeled_text_dataset.append(labeled_text)\n",
        "        else:\n",
        "            labeled_text_dataset.append((page[\"taskId\"], labeled_text))\n",
        "\n",
        "    return labeled_text_dataset"
      ],
      "metadata": {
        "id": "p9KN1olq_s0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def get_labeled_dataset(dataset_path, for_inference=False):\n",
        "    with open(dataset_path) as json_dataset:\n",
        "        dataset = json.load(json_dataset)\n",
        "\n",
        "    labeled_dataset = transform_dataset(dataset[\"data\"][\"results\"], for_inference)\n",
        "    return labeled_dataset"
      ],
      "metadata": {
        "id": "3xRfSPcy_tp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_validation_result(X_validation, y_pred):\n",
        "    validation_result = []\n",
        "\n",
        "    for ((task_id, _), predictions) in zip(X_validation, y_pred):\n",
        "        answers = {}\n",
        "        current_label = None\n",
        "        start_index = None\n",
        "\n",
        "        for current_index, label in enumerate(predictions):\n",
        "            if label == current_label:\n",
        "                continue\n",
        "            else:\n",
        "                if current_label is not None and current_label != \"O\":\n",
        "                    if current_label not in answers:\n",
        "                        answers[current_label] = []\n",
        "                    answers[current_label].append({\"start\": start_index, \"end\": current_index})\n",
        "\n",
        "                if label != \"0\":\n",
        "                    current_label = label\n",
        "                    start_index = current_index\n",
        "                else:\n",
        "                    current_label = None\n",
        "\n",
        "        if current_label is not None and current_label != \"O\":\n",
        "            if current_label not in answers:\n",
        "                answers[current_label] = []\n",
        "            answers[current_label].append({\"start\": start_index, \"end\": len(predictions)})\n",
        "\n",
        "        validation_answers = []\n",
        "        for label, segments in answers.items():\n",
        "            validation_answers.append({\"id\": label, \"data\": segments})\n",
        "\n",
        "        validation_result.append({\n",
        "            \"taskId\": task_id,\n",
        "            \"answer\": validation_answers\n",
        "        })\n",
        "\n",
        "    return validation_result"
      ],
      "metadata": {
        "id": "4t_b2Vs8_xtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def focal_loss(alpha=0.25, gamma=2.):\n",
        "    def focal_loss_parametrized(y_true, y_pred):\n",
        "        e = 1.e-9\n",
        "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
        "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
        "\n",
        "        model_output = tf.add(y_pred, e)\n",
        "        ce = tf.multiply(y_true, -tf.math.log(model_output))\n",
        "        w = tf.multiply(y_true, tf.pow(tf.subtract(1., model_output), gamma))\n",
        "        fl = tf.multiply(alpha, tf.multiply(w, ce))\n",
        "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
        "        return tf.reduce_mean(reduced_fl)\n",
        "\n",
        "    return focal_loss_parametrized"
      ],
      "metadata": {
        "id": "hwdRl17I_1zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_inference(batch_size):\n",
        "    predictions = model.predict(X_test, batch_size=batch_size, use_multiprocessing=True, workers=os.cpu_count())\n",
        "\n",
        "    y_pad_pred_test = [[labels[np.argmax(prediction)] for prediction in text_prediction]\n",
        "                      for text_prediction in predictions]\n",
        "\n",
        "    y_pred_test = []\n",
        "\n",
        "    for i, text in enumerate(test_dataset):\n",
        "        text_predictions = []\n",
        "        for j, (word, _) in enumerate(text):\n",
        "            if j < len(y_pad_pred_test[i]):\n",
        "                text_predictions.append((word, y_pad_pred_test[i][j]))\n",
        "\n",
        "        y_pred_test.append(text_predictions)\n",
        "\n",
        "    y_test_flat = [label for text in test_dataset for _, label in text]\n",
        "    y_pred_flat = [label for text in y_pred_test for _, label in text]\n",
        "\n",
        "    return y_test_flat, y_pred_flat"
      ],
      "metadata": {
        "id": "qhPkvt7V5wIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "train_dataset = get_labeled_dataset(\"/content/drive/My Drive/Colab Notebooks/datasets/train_geo_extractor.json\")\n",
        "test_dataset = get_labeled_dataset(\"/content/drive/My Drive/Colab Notebooks/datasets/test_geo_extractor.json\")\n",
        "validation_dataset = get_labeled_dataset(\"/content/drive/My Drive/Colab Notebooks/datasets/val_no_answer_geo_extractor.json\",\n",
        "                                         for_inference=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lFZTJSf_5DB",
        "outputId": "2b67d09a-b30d-4721-fa96-d6e8954e6d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_text_length = max([len(text) for text in train_dataset])\n",
        "\n",
        "words = [word for text in train_dataset for word, _ in text]\n",
        "words.append(\"UNKNOWN\")\n",
        "words.append(\"ENDPAD\")\n",
        "words = list(set(words))\n",
        "\n",
        "labels = list(set([label for text in train_dataset for _, label in text]))"
      ],
      "metadata": {
        "id": "Sr2zOf8T_9jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2index = {word: index for index, word in enumerate(words)}\n",
        "label2index = {label: index for index, label in enumerate(labels)}"
      ],
      "metadata": {
        "id": "URd07YsvAAps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "X_train = [[word2index[word] for word, _ in text] for text in train_dataset]\n",
        "X_train = pad_sequences(maxlen=max_text_length, sequences=X_train, padding=\"post\", value=len(word2index) - 1)\n",
        "\n",
        "y_train = [[label2index[label] for _, label in text] for text in train_dataset]\n",
        "y_train = pad_sequences(maxlen=max_text_length, sequences=y_train, padding=\"post\", value=label2index[\"O\"])\n",
        "y_train = [to_categorical(index, num_classes=len(label2index)) for index in y_train]"
      ],
      "metadata": {
        "id": "vNTEo31IACwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = [[word2index.get(word, word2index[\"UNKNOWN\"]) for word, _ in text] for text in test_dataset]\n",
        "X_test = pad_sequences(maxlen=max_text_length, sequences=X_test, padding=\"post\", value=len(word2index) - 1)"
      ],
      "metadata": {
        "id": "PMFCvY4ynHGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class ComputeMCC(Callback):\n",
        "    def __init__(self):\n",
        "        super(ComputeMCC, self).__init__()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_test_flat, y_pred_flat = test_inference(batch_size=512)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}. MCC: {matthews_corrcoef(y_test_flat, y_pred_flat)}\")"
      ],
      "metadata": {
        "id": "821zAO8IxGW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dropout, Conv1D, Dense\n",
        "\n",
        "model_input = Input(shape=(max_text_length, ))\n",
        "embedding_output = Embedding(input_dim=len(word2index), output_dim=max_text_length, input_length=max_text_length)(model_input)\n",
        "\n",
        "dropout_output = Dropout(0.5)(embedding_output)\n",
        "conv1d_output = Conv1D(filters=300, kernel_size=3, padding='same', activation='relu')(dropout_output)\n",
        "\n",
        "dropout_output = Dropout(0.3)(conv1d_output)\n",
        "conv1d_output = Conv1D(filters=200, kernel_size=3, padding='same', activation='relu')(dropout_output)\n",
        "\n",
        "dropout_output = Dropout(0.1)(conv1d_output)\n",
        "conv1d_output = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu')(dropout_output)\n",
        "\n",
        "model_output = Dense(len(label2index), activation=\"softmax\")(conv1d_output)\n",
        "\n",
        "model = Model(model_input, model_output)\n",
        "model.compile(optimizer=\"adam\", loss=focal_loss(), metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(X_train, np.array(y_train), batch_size=512, epochs=15, callbacks=[ComputeMCC()],\n",
        "          use_multiprocessing=True, workers=os.cpu_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm8-SjghAoGH",
        "outputId": "49485745-5c3b-4b4c-b699-b83247abc7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "2/2 [==============================] - 10s 4s/step\n",
            "Epoch 1. MCC: 0.35735998684330006\n",
            "6/6 [==============================] - 41s 4s/step - loss: 0.2049 - accuracy: 0.9159\n",
            "Epoch 2/15\n",
            "2/2 [==============================] - 0s 85ms/step\n",
            "Epoch 2. MCC: 0.5976594902141319\n",
            "6/6 [==============================] - 4s 802ms/step - loss: 0.1525 - accuracy: 0.9914\n",
            "Epoch 3/15\n",
            "2/2 [==============================] - 0s 84ms/step\n",
            "Epoch 3. MCC: 0.6147680821649801\n",
            "6/6 [==============================] - 4s 807ms/step - loss: 0.1256 - accuracy: 0.9945\n",
            "Epoch 4/15\n",
            "2/2 [==============================] - 0s 83ms/step\n",
            "Epoch 4. MCC: 0.6701884496014346\n",
            "6/6 [==============================] - 4s 805ms/step - loss: 0.1094 - accuracy: 0.9949\n",
            "Epoch 5/15\n",
            "2/2 [==============================] - 0s 83ms/step\n",
            "Epoch 5. MCC: 0.7077121169699299\n",
            "6/6 [==============================] - 4s 798ms/step - loss: 0.0925 - accuracy: 0.9954\n",
            "Epoch 6/15\n",
            "2/2 [==============================] - 0s 84ms/step\n",
            "Epoch 6. MCC: 0.772902138595918\n",
            "6/6 [==============================] - 4s 800ms/step - loss: 0.0766 - accuracy: 0.9960\n",
            "Epoch 7/15\n",
            "2/2 [==============================] - 0s 84ms/step\n",
            "Epoch 7. MCC: 0.7933368943862786\n",
            "6/6 [==============================] - 4s 803ms/step - loss: 0.0623 - accuracy: 0.9967\n",
            "Epoch 8/15\n",
            "2/2 [==============================] - 0s 84ms/step\n",
            "Epoch 8. MCC: 0.813086135560299\n",
            "6/6 [==============================] - 4s 799ms/step - loss: 0.0494 - accuracy: 0.9973\n",
            "Epoch 9/15\n",
            "2/2 [==============================] - 0s 84ms/step\n",
            "Epoch 9. MCC: 0.8255154100446584\n",
            "6/6 [==============================] - 4s 804ms/step - loss: 0.0392 - accuracy: 0.9978\n",
            "Epoch 10/15\n",
            "2/2 [==============================] - 0s 84ms/step\n",
            "Epoch 10. MCC: 0.8365629718771677\n",
            "6/6 [==============================] - 4s 807ms/step - loss: 0.0307 - accuracy: 0.9983\n",
            "Epoch 11/15\n",
            "2/2 [==============================] - 0s 85ms/step\n",
            "Epoch 11. MCC: 0.8392065156194172\n",
            "6/6 [==============================] - 4s 800ms/step - loss: 0.0247 - accuracy: 0.9986\n",
            "Epoch 12/15\n",
            "2/2 [==============================] - 0s 84ms/step\n",
            "Epoch 12. MCC: 0.8442358369609005\n",
            "6/6 [==============================] - 4s 804ms/step - loss: 0.0201 - accuracy: 0.9989\n",
            "Epoch 13/15\n",
            "2/2 [==============================] - 0s 85ms/step\n",
            "Epoch 13. MCC: 0.8443766485649706\n",
            "6/6 [==============================] - 4s 803ms/step - loss: 0.0170 - accuracy: 0.9990\n",
            "Epoch 14/15\n",
            "2/2 [==============================] - 0s 85ms/step\n",
            "Epoch 14. MCC: 0.8485222075669824\n",
            "6/6 [==============================] - 4s 800ms/step - loss: 0.0148 - accuracy: 0.9992\n",
            "Epoch 15/15\n",
            "2/2 [==============================] - 0s 85ms/step\n",
            "Epoch 15. MCC: 0.8492536796133454\n",
            "6/6 [==============================] - 4s 805ms/step - loss: 0.0127 - accuracy: 0.9993\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c1afff0e890>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, matthews_corrcoef\n",
        "\n",
        "y_test_flat, y_pred_flat = test_inference(batch_size=512)\n",
        "\n",
        "print(classification_report(y_test_flat, y_pred_flat))\n",
        "print(f\"Matthews Correlation Coefficient: {matthews_corrcoef(y_test_flat, y_pred_flat)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysL3oLx3BK7u",
        "outputId": "51ed415a-9970-422e-c6a3-a925a161b5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 85ms/step\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "                O       0.99      0.99      0.99     62822\n",
            "     central_city       0.45      0.32      0.37       184\n",
            "      geo_address       0.85      0.65      0.74      1040\n",
            "     geo_building       0.78      0.77      0.77       453\n",
            "         geo_city       0.83      0.78      0.81      1433\n",
            "     geo_district       0.82      0.77      0.79       387\n",
            "geo_microdistrict       0.62      0.58      0.60       382\n",
            "       geo_region       0.99      0.98      0.99      1733\n",
            "geo_region_oblast       0.87      0.78      0.82       297\n",
            "       geo_street       0.70      0.76      0.73      1059\n",
            "\n",
            "         accuracy                           0.97     69790\n",
            "        macro avg       0.79      0.74      0.76     69790\n",
            "     weighted avg       0.97      0.97      0.97     69790\n",
            "\n",
            "Matthews Correlation Coefficient: 0.8492536796133454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_validation = [[word2index.get(word, word2index[\"UNKNOWN\"]) for word, _ in text]\n",
        "                for task_id, text in validation_dataset]\n",
        "X_validation = pad_sequences(maxlen=max_text_length, sequences=X_validation, padding=\"post\",\n",
        "                             value=len(word2index) - 1)"
      ],
      "metadata": {
        "id": "Ta5dQXM0BUxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_validation = model.predict(X_validation, batch_size=512, use_multiprocessing=True, workers=os.cpu_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxmY5cANBZ1-",
        "outputId": "9a5fbc1e-58aa-48bd-ca38-1d7219812e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 5s 965ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_validation = [(task_id, text) for task_id, text in validation_dataset]\n",
        "\n",
        "y_pad_pred_validation = [[labels[np.argmax(prediction)] for prediction in text_prediction]\n",
        "              for text_prediction in y_pred_validation]\n",
        "\n",
        "y_pred_validation = []\n",
        "\n",
        "for i, text in enumerate(validation_dataset):\n",
        "    text_predictions = []\n",
        "    for j, (word, _) in enumerate(text[1]):\n",
        "        if j < len(y_pad_pred_validation[i]):\n",
        "            text_predictions.append((word, y_pad_pred_validation[i][j]))\n",
        "\n",
        "    y_pred_validation.append(text_predictions)"
      ],
      "metadata": {
        "id": "mS6ACuWWBcwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "y_pred_validation = [[label for _, label in text] for text in y_pred_validation]\n",
        "\n",
        "validation_result = get_validation_result(X_validation, y_pred_validation)\n",
        "\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/hybrid_validation_result.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(validation_result, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"Validation result has been saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtFzV3oDBiGy",
        "outputId": "fc384831-8072-4075-8b67-ec707f8054dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation result has been saved!\n"
          ]
        }
      ]
    }
  ]
}